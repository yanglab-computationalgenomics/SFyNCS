#!/bin/bash

# 2023-02-14
# version: v0.14

# default parameter
thread_number=1
current_directory=$PWD
output_directory=$PWD
adjust_adjacent_distance=5
cluster_distance=1000000
min_split_reads=1
min_read_pairs=1
min_total_reads=3
overhang_length=5
read_pair_distance=10000
motif_searching_length_in_blat=5
outside_length_in_blat=1000000
inside_length_in_blat=100
length_for_identity_in_blat=10
align_percentage_in_blat=0.9
max_split_read_blat_distance=10000000
max_sequence_identity_in_blat=1
filter_by_canonical_splice_motif="Y"
length_in_sd=100
sd_cutoff=0.1
filter_in_the_same_gene="Y"
normal_adjacent_distance=10000
normal_read_count_cutoff=2
deletion_like_distance=500000
duplication_like_and_inversion_like_distance=50000

usage(){
 cat << EOF
Description:
    This script was used to identify fusion transcripts from pair-end RNA-seq data (version: v0.14).
        
Usage ([optional options] <must be provided options>):
    $0 [-p 1 -o output -c Chimeric.out.junction -s star_index] <-a annotation_file> <-g genome_fasta> <-t tophat_index> <read_1.fastq|read_1.fastq.gz> <read_2.fastq|read_2.fastq.gz> 
    
Options:
    -a  --annotation_file                   STR     Gene annotation gpe file
    -g  --genome_fasta                      STR     Reference genome fasta file
    -o  --output_directory                  STR     Output directory [default: current directory]
    -p  --thread_number                     INT     Number of threads [default: $thread_number]. Multiple threads can speed up the steps of STAR and TopHat
    -s  --star_index                        STR     Path to STAR index. This option can be skipped if "-c" is provided
    -t  --tophat_index                      STR     Path to TopHat index. It includes the name of any of index files up to but not including the first period
    -c  --chimeric_file                     STR     Chimeric.out.junction file generated by STAR.
    -d  --normal_junction_dir               STR     Directory contains normal samples' junctions
        --adjust_adjacent_distance          INT     Breakpoints within this distance will be adjusted [default: $adjust_adjacent_distance]
        --cluster_distance                  INT     Split reads and read pairs within this distance will be clustered together [default: $cluster_distance]
        --min_split_reads                   INT     Minimal number of split reads for a fusion transcript to be identified [default: $min_split_reads]
        --min_read_pairs                    INT     Minimal number of read pairs for a fusion transcript to be identified [default: $min_read_pairs]
        --min_total_reads                   INT     Minimal number of total reads for a fusion transcript to be identified [default: $min_total_reads]
        --overhang_length                   INT     Breakpoint overhang length for TopHat [default: $overhang_length]
        --read_pair_distance                INT     Maximal distance between the TopHat alignment of read pairs and breakpoint [default: $read_pair_distance]      
        --motif_searching_length_in_blat    INT     Splice site motifs (GT in the donor, AAG/CAG/TAG in the acceptor) are searched within this window size of breakpoints [default: $motif_searching_length_in_blat]
        --outside_length_in_blat           INT     Specify the window of breakpoint flanking sequence for artificial reference [default: $outside_length_in_blat]
        --inside_length_in_blat           INT     Specify the window of breakpoint flanking sequence for artificial reference [default: $inside_length_in_blat]
        --length_for_identity_in_blat       INT     Flanking sequences size of both fusion breakpoints to calculate sequence identity [default: $length_for_identity_in_blat]
        --align_percentage_in_blat          FLOAT   Minimal percentage of bases of the whole read that is alignable by Blat for split reads [default: $align_percentage_in_blat]
        --max_split_read_blat_distance      INT     Maximal distance between Blat alignment and breakpoints [default: $max_split_read_blat_distance]
        --max_sequence_identity_in_blat     INT     Maximal sequence identity between flanking sequences of two fusion breakpoints [default: $max_sequence_identity_in_blat]
        --filter_by_canonical_splice_motif  STR     Filter by canonical splice site motif [default: $filter_by_canonical_splice_motif]
        --length_in_sd                      INT     Breakpoint flanking size to calculate standard deviation [default: $length_in_sd]
        --sd_cutoff                         FLOAT   Standard deviation cutoff to filter fusions [default: $sd_cutoff]
        --filter_in_the_same_gene           STR     Filter fusions in the same gene [default: $filter_in_the_same_gene]
        --normal_adjacent_distance          INT     Window size to search for breakpoints in normal samples [default: $normal_adjacent_distance]
        --normal_read_count_cutoff          INT     Filter fusions if numbers of reads (discordant pairs or split reads) in normal samples are equal to or more than the specified value [default: $normal_read_count_cutoff]
        --deletion_like_distance           INT     Minimal distance allowed between fusion breakpoints if located in the same chromosome and strand is plus (smaller coordinate) minus (larger coordinate) [default: $deletion_like_distance]
        --duplication_like_and_inversion_like_distance       INT     Minimal distance allowed between breakpoints if located in the same chromosome and strand is not plus minus [default: $duplication_like_and_inversion_like_distance]
    -h  --help                                      Print this help menu.

Gene annotation format (must have header, start position is 0-base, end position is 1-base):
    Transcript_id           Chr     Strand   Transcript_start_position  Transcript_end_position CDS_start_position  CDS_end_position  Exon_count    Exon_starts                     Exon_ends                     Score   Symbol  CDS_start_stat  CDS_end_stat  Exon_frame
    ENST00000485503.1       chr7    +         55192810                  55200802                55200802            55200802          3             55192810,55198716,55200315,     55192841,55198863,55200802,   0       EGFR    none            none          -1,-1,-1,

Chimeric.out.junction format (didn't have header, refer to https://physiology.med.cornell.edu/faculty/skrabanek/lab/angsd/lecture_notes/STARmanual.pdf for more detail):
    Chromosome_of_the_donor First_base_of_the_intron_of_the_donor   Strand_of_the_donor Chromosome_of_the_acceptor  First_base_of_the_intron_of_the_acceptor    Strand_of_the_acceptor   Junction_type  Repeat_length_to_the_left_of_the_junction   Repeat_length_to_the_right_of_the_junction  Read_name                       First_base_of_the_first_segment CIGAR_of_the_first_segment  First_base_of_the_second_segment    CIGAR_of_the_second_segment
    chr22                   23632601                                +                   chr9                        133729450                                   +                        1              0                                           0                                           SINATRA-0006:3:3:6387:5665#0    23632554                        47M29S                      133729451                           47S29M40p76M
EOF
    exit 0
}

[ $# -eq 0 ] && usage

declare -A options=( [h]=help [a:]=annotation_file: [g:]=genome_fasta: [o:]=output_directory: [p:]=thread_number: [s:]=star_index: [t:]=tophat_index: [c:]=chimeric_file: 
    [d:]=normal_junction_dir: [0:]=adjust_adjacent_distance: [1:]=cluster_distance: [2:]=min_split_reads: [3:]=min_read_pairs: [4:]=min_total_reads: [5:]=overhang_length: 
    [6:]=read_pair_distance: [7:]=motif_searching_length_in_blat: [8:]=outside_length_in_blat: [9:]=inside_length_in_blat: 
    [10:]=length_for_identity_in_blat: [11:]=align_percentage_in_blat: [12:]=max_split_read_blat_distance: [13:]=max_sequence_identity_in_blat: [14:]=filter_by_canonical_splice_motif: 
    [15:]=length_in_sd: [16:]=sd_cutoff: [17:]=filter_in_the_same_gene: [18:]=normal_adjacent_distance: [19:]=normal_read_count_cutoff: [20:]=deletion_like_distance: [21:]=duplication_like_and_inversion_like_distance: )

# create string of short options
opt_short=$(printf "%s" "${!options[@]}")

# create string of long options
opt_long="$(printf ",%s" "${options[@]}")"

# catch wrong options and move non-options to the end of the string
args=$(getopt -l "$opt_long" "$opt_short" "$@" 2> >(sed -e 's/^/stderr/g')) || echo -n "Error: " && echo "$args" | grep -oP "(?<=^stderr).*" && exit 1

# create new array of options
mapfile -t args < <(xargs -n1 <<< "$(echo "$args" | sed -E "s/(--[^ ]+) /\1=/g")" )

# overwrite $@ (options)
set -- "${args[@]}"

# parse options ([h]=help sets the variable "$opt_help" and [V]="" sets the variable "$opt_V")
while getopts "$opt_short-:" opt;
    do
        # long option
        if [[ "$opt" == "-" ]]; then
            # extract long option name
            opt="${OPTARG%%=*}"
    
            # extract long option argument (may be empty)
            OPTARG="${OPTARG#"$opt"}"
    
            # remove "=" from long option argument
            OPTARG="${OPTARG#=}"
    
            # set variable name
            opt=$opt
        # short option without argument uses long option name as variable name
        elif [[ "${options[$opt]+x}" ]] && [[ "${options[$opt]}" ]]; then
            opt=${options[$opt]} 
        # short option with argument uses long option name as variable name
        elif [[ "${options[$opt:]+x}" ]] && [[ "${options[$opt:]}" ]]; then
            opt=${options[$opt:]} 
        # short option without long option name uses short option name as variable name
        else
            opt=$opt
        fi
        
        # remove double colon
        opt="${opt%:}"
    
        # options without arguments are set to 1 (this is didn't impact this script)
        [[ ! $OPTARG ]] && OPTARG=1
        
        # set variable variables
        printf -v "$opt" '%s' "$OPTARG" 
    done
# remove all parsed options from $@
shift $((OPTIND-1))

[ ! -z $help ] && usage
[ -z $annotation_file ] && echo "Please provide gene annotation with -a" && usage
[ -z $genome_fasta ] && echo "Please provide genome fasta with -g" && usage
([ -z $star_index ] && [ -z $chimeric_file ]) && echo "Please provide STAR index with -s or Chimeric.out.junction with -c" && usage 
[ -z $tophat_index ] && echo "Please provide Tophat index with -t" && usage
[ -z $1 ] && echo "Please provide fastq" && usage
[ -z $2 ] && echo "Please provide fastq" && usage

function runningTime(){
    startS=$(echo $1 | awk -F: '{ print ($1 * 3600) + ($2 * 60) + $3 }') 
    endS=$(echo $2 | awk -F: '{ print ($1 * 3600) + ($2 * 60) + $3 }') 
    runTime=$(( $endS - $startS ))
    date -d@${runTime} -u '+%H:%M:%S'
}

startTime=$( date +%H:%M:%S)
echo -e "START:\t$startTime"


# Generate output directory
fastq_1=$(readlink -f $1)
fastq_2=$(readlink -f $2)
annotation_file=$(readlink -f $annotation_file)
genome_fasta=$(readlink -f $genome_fasta)
star_index=$(readlink -f $star_index)
tophat_index=$(readlink -f $tophat_index)
toolDir=$(readlink -f $0 | xargs dirname)
if [ ! -z $chimeric_file ]; then
    chimeric_file=$(readlink -f $chimeric_file)
fi
if [ ! -z $normal_junction_dir ]; then
    normal_junction_dir=$(readlink -f $normal_junction_dir)
fi
output_directory=$(readlink -f $output_directory)
[ -e ${output_directory}/temp_output_SFyNCS ] && rm -rf ${output_directory}/temp_output_SFyNCS
mkdir -p ${output_directory}/temp_output_SFyNCS && cd ${output_directory}/temp_output_SFyNCS
if echo $fastq_1 | grep -q ".gz$"; then
    ln -s $fastq_1 1.fastq.gz
    ln -s $fastq_2 2.fastq.gz
else
    ln -s $fastq_1 1.fastq
    ln -s $fastq_2 2.fastq
fi


# 1. Align fastq with STAR
echo -e "Step 1: Generate Chimeric.out.junction by running STAR or make a soft link to provided Chimeric.out.junction"
mkdir star_output
if [ -z $chimeric_file ]; then
    star_common_command="--outReadsUnmapped None  \
        --twopassMode Basic \
        --outSAMstrandField intronMotif  \
        --outSAMunmapped Within  \
        --chimSegmentMin 12  \
        --chimJunctionOverhangMin 12  \
        --chimOutJunctionFormat 1  \
        --alignSJDBoverhangMin 10  \
        --alignMatesGapMax 100000  \
        --alignIntronMax 100000  \
        --alignSJstitchMismatchNmax 5 -1 5 5  \
        --outSAMattrRGline ID:GRPundef  \
        --chimMultimapScoreRange 10 \
        --chimMultimapNmax 10 \
        --chimNonchimScoreDropMin 10  \
        --peOverlapNbasesMin 12 \
        --peOverlapMMp 0.1  \
        --outSAMtype BAM SortedByCoordinate \
        --genomeLoad NoSharedMemory"
    if echo $fastq_1 | grep -q ".gz$"; then
        STAR --genomeDir $star_index --runThreadN $thread_number --outFileNamePrefix star_output/ $star_common_command --readFilesIn 1.fastq.gz 2.fastq.gz --readFilesCommand zcat
    else
        STAR --genomeDir $star_index --runThreadN $thread_number --outFileNamePrefix star_output/ $star_common_command --readFilesIn 1.fastq 2.fastq
    fi
else
    ln -s $chimeric_file $PWD/star_output/Chimeric.out.junction
fi

discordant_count=$(grep -v "^#" star_output/Chimeric.out.junction | wc -l)
if [ $discordant_count -eq 0 ]; then
    cd ${output_directory}  
    echo -e "Chr_breakpoint_1\tPos_breakpoint_1\tStrand_breakpoint_1\tChr_breakpoint_2\tPos_breakpoint_2\tStrand_breakpoint_2\t \
        Split_read_count_(star)\tRead_pair_count_(star)\tSplit_read_count_(tophat)\tPotential_split_read_count_(tophat)\tRead_pair_count_(tophat)\t \
        Split_read_count_(blat_tophat_split_and_tophat_potential_split_reads)\tSplit_read_count_(blat_tophat_split_reads)\t \
        Minimum_read_pair_distance_to_breakpoint_1\tMinimum_read_pair_distance_to_breakpoint_2\t \
        Sequence_identity\tMinimum_blat_distance_to_breakpoint_1_(tophat_split_and_potential_split_reads)\tMinimum_blat_distance_to_breakpoint_2_(tophat_split_and_potential_split_reads)\t \
        Minimum_blat_distance_to_breakpoint_1_(tophat_split_reads)\tMinimum_blat_distance_to_breakpoint_2_(tophat_split_reads)\t \
        Have_canonical_motif\t \
        Cluster_count_breakpoint_1\tCluster_count_breakpoint_2\tCluster_count_breakpoint_1_and_2\t \
        (discordant_reads)%_support_fusion\t \
        SD_(discordant_reads)%_in_breakpoint_1\tSD_(discordant_reads)%_in_breakpoint_2\t \
        Whether_in_the_same_gene\t \
        Fusion_type\t \
        Gene_name_breakpoint_1\tGene_type_breakpoint_1\tGene_strand_breakpoint_1\tBreakpoint_location_breakpoint_1\tBreakpoint_region_type_breakpoint_1\tFrame_extra_base_breakpoint_1\t \
        Gene_name_breakpoint_2\tGene_type_breakpoint_2\tGene_strand_breakpoint_2\tBreakpoint_location_breakpoint_2\tBreakpoint_region_type_breakpoint_2\tFrame_extra_base_breakpoint_2\t \ 
        Fusion_frame\t \
        Split_reads_(star)\tRead_pairs_(star)\tSplit_reads_(tophat)\tPotential_split_reads_(tophat)\tRead_pairs_(tophat)\t \
        Split_reads_(blat_tophat_split_and_tophat_potential_split_reads)\t \
        Read_pair_distance_to_breakpoint_1\tRead_pair_distance_to_breakpoint_2\t \
        Blat_distance_to_breakpoint_1_(tophat_split_and_tophat_potential_split_reads)\tBlat_distance_to_breakpoint_2_(tophat_split_and_tophat_potential_split_reads)\t \
        Blat_distance_to_breakpoint_1_(tophat_split_reads)\tBlat_distance_to_breakpoint_2_(tophat_split_eads)\t \
        Sequence_identity_alignment_breakpoint_1\tSequence_identity_alignment_breakpoint_2\t \
        Cluster_ids_breakpoint_1\tCluster_ids_breakpoint_2\t \
        Read_count_in_each_cluster_breakpoint_1\tRead_count_in_each_cluster_breakpoint_2" | sed "s# ##g" >fusions.tsv
    awk 'BEGIN{FS=OFS="\t"} {print $1,$2,$3,$4,$5,$6,$12,$11,$29,$30,$31,$32,$36,$37,$38,$42;}' fusions.tsv >fusions_abridged.tsv
    [ -e fusions.tsv.gz ] && rm fusions.tsv.gz
    [ -e fusions_abridged.tsv.gz ] && rm fusions_abridged.tsv.gz
    gzip fusions.tsv
    gzip fusions_abridged.tsv
    echo "Finished!" && rm -rf ${output_directory}/temp_output_SFyNCS && exit
fi


# 2. Format and generate preliminary fusions
echo -e "\nStep 2: Generating preliminary fusions"
perl $toolDir/format_STAR_chimeric_file.pl  star_output/Chimeric.out.junction >format_chimeric.tsv
perl $toolDir/remove_multiple_mapped_reads.pl format_chimeric.tsv >no_multiple_mapped.tsv
perl $toolDir/remove_duplicate_reads.pl no_multiple_mapped.tsv >temp_no_duplicate.tsv # sort may put Chr_breakpoint_1 to last line, so take 2 steps below
# sort may put Chr_breakpoint_1 to last line, so take two steps
head -n 1 temp_no_duplicate.tsv >no_duplicate.tsv
sort temp_no_duplicate.tsv | uniq | grep -v "Chr_breakpoint_1" >>no_duplicate.tsv
rm temp_no_duplicate.tsv
perl $toolDir/adjust_ajacent_breakpoints.pl -a $adjust_adjacent_distance no_duplicate.tsv >adjust_ajacent.tsv
perl $toolDir/cluster_discordant_reads.pl -w $cluster_distance adjust_ajacent.tsv >cluster.tsv
# generate temp_cluster.bed, which was used in calculating SD
awk 'BEGIN{FS=OFS="\t"} NR>1{print $1,$2-1,$2,$3,$7,$8,$9; print $4,$5-1,$5,$6,$7,$8,$9;}' cluster.tsv | sort -k1,1 -k2,2n | uniq >temp_cluster.bed
perl $toolDir/identify_fusion_candidates_from_cluster_reads.pl cluster.tsv >preliminary_candidates.tsv
# filter by read count to speed up
awk -v min_split_reads=$min_split_reads -v min_read_pairs=$min_read_pairs -v min_total_reads=$min_total_reads 'NR==1 || ($9>=min_split_reads && $10>=min_read_pairs && ($9+$10)>=min_total_reads)' preliminary_candidates.tsv >temp.tsv
mv temp.tsv preliminary_candidates.tsv
# filter by fusion distance to speed up
awk -v deletion_like_distance=$deletion_like_distance -v duplication_like_and_inversion_like_distance=$duplication_like_and_inversion_like_distance '$1!=$4{print $0;} $1==$4{if($3=="+" && $6=="-"){if(($5-$2)>=deletion_like_distance) print $0;} else{if(($5-$2)>=duplication_like_and_inversion_like_distance ) print $0;}}' preliminary_candidates.tsv >temp.tsv
mv temp.tsv preliminary_candidates.tsv
# remove chrMT (this step can be dropped in further version)
grep -v "chrM" preliminary_candidates.tsv >temp.tsv
mv temp.tsv preliminary_candidates.tsv
rm cluster.tsv format_chimeric.tsv adjust_ajacent.tsv no_duplicate.tsv no_multiple_mapped.tsv
rm -rf star_output


# 3. process preliminary fusions with tophat
echo -e "\nStep 3: Processing with Tophat"
# extract prelimary fusions' discordant reads to speed up, align them with tophat 
cut -f11,12 preliminary_candidates.tsv | sed "s#\t#\n#;s#,#\n#g" | grep -vw "NA" | grep -vP "Split_reads|Read_pairs" | sort | uniq >selected_discordant_reads.tsv
perl $toolDir/select_fastq.pl -s selected_discordant_reads.tsv 1.fastq* 2.fastq* >selected_discordant_reads_1.fastq 2>selected_discordant_reads_2.fastq
rm selected_discordant_reads.tsv

tophat --no-coverage-search \
    --fusion-search \
    --fusion-anchor-length 12 \
    --fusion-min-dist 100000 \
    --read-mismatches 4 \
    --read-gap-length 4 \
    --read-edit-dist 4 \
    --splice-mismatches 2 \
    --max-insertion-length 4 \
    --max-deletion-length 4 \
    --segment-mismatches 3 \
    --fusion-read-mismatches 4 \
    -o tophat_output \
    -p $thread_number \
    $tophat_index \
    selected_discordant_reads_1.fastq selected_discordant_reads_2.fastq
ln -s tophat_output/accepted_hits.bam tophat.bam
samtools index tophat.bam
perl $toolDir/processe_by_tophat.pl -o $overhang_length -w $cluster_distance -d $read_pair_distance -p $min_split_reads -r $min_read_pairs -t $min_total_reads tophat.bam preliminary_candidates.tsv >processed_with_tophat.tsv
rm -rf tophat* preliminary_candidates.tsv 


# 4. process preliminary fusions with blat
echo -e "\nStep 4: Processing with Blat"
# split file to 50 chunk to redunce memory and speed up processing
lineCount=$( wc -l processed_with_tophat.tsv | cut -f1 -d " " )
chunk=$( echo "$lineCount/50" | bc )
for((i=1; i<50; i++))
    do
        mkdir chunk_${i}
        head -n 1 processed_with_tophat.tsv >chunk_${i}/processed_with_tophat.tsv
        line_start=$[ ($i-1)*$chunk+1 ]
        line_end=$[ $i*chunk ]
        if [ $chunk -gt 0 ]; then
            sed -n "${line_start},${line_end}p" processed_with_tophat.tsv >>chunk_${i}/processed_with_tophat.tsv
        fi
    done
if [ $chunk -gt 0 ]; then
    sed -i '1d' chunk_1/processed_with_tophat.tsv
fi
line_start=$[ 49*$chunk+1 ]
line_end=$lineCount
mkdir chunk_50
if [ $chunk -gt 0 ]; then
    head -n 1 processed_with_tophat.tsv >chunk_50/processed_with_tophat.tsv
fi
sed -n "${line_start},${line_end}p" processed_with_tophat.tsv >>chunk_50/processed_with_tophat.tsv

for i in {1..50}
    do
        cd chunk_${i}
        ln -s ../selected_discordant_reads_1.fastq .
        ln -s ../selected_discordant_reads_2.fastq .
        perl $toolDir/processe_by_blat.pl -f $genome_fasta -m $motif_searching_length_in_blat -c $filter_by_canonical_splice_motif -a $align_percentage_in_blat -o $outside_length_in_blat -i $inside_length_in_blat -p $min_split_reads -t $min_total_reads -d $max_split_read_blat_distance -l $length_for_identity_in_blat -e $max_sequence_identity_in_blat processed_with_tophat.tsv >processed_with_blat.tsv
        cd ..
    done

cp chunk_1/processed_with_blat.tsv .
for((i=2; i<51; i++))
    do
        sed -n '2,$p' chunk_${i}/processed_with_blat.tsv >>processed_with_blat.tsv
    done
rm -rf chunk*
rm selected_discordant_reads_1.fastq selected_discordant_reads_2.fastq


# 5. generating fusion statistics
echo -e "\nStep 5: Generating fusion statistics"
# split file to 50 chunk to redunce memory and speed up processing
lineCount=$( wc -l processed_with_blat.tsv | cut -f1 -d " " )
chunk=$( echo "$lineCount/50" | bc )
for((i=1; i<50; i++))
    do
        mkdir chunk_${i}
        head -n 1 processed_with_blat.tsv >chunk_${i}/processed_with_blat.tsv
        line_start=$[ ($i-1)*$chunk+1 ]
        line_end=$[ $i*chunk ]
        if [ $chunk -gt 0 ]; then
            sed -n "${line_start},${line_end}p" processed_with_blat.tsv >>chunk_${i}/processed_with_blat.tsv
        fi
    done
if [ $chunk -gt 0 ]; then
    sed -i '1d' chunk_1/processed_with_blat.tsv
fi
line_start=$[ 49*$chunk+1 ]
line_end=$lineCount
mkdir chunk_50
if [ $chunk -gt 0 ]; then
    head -n 1 processed_with_blat.tsv >chunk_50/processed_with_blat.tsv
fi
sed -n "${line_start},${line_end}p" processed_with_blat.tsv >>chunk_50/processed_with_blat.tsv

for i in {1..50}
    do
        cd chunk_${i}
        ln -s ../temp_cluster.bed .
        perl $toolDir/cluster_statistics.pl -f $length_in_sd -s $sd_cutoff processed_with_blat.tsv >fusion_statistics.tsv
        cd ..
    done

cp chunk_1/fusion_statistics.tsv .
for((i=2; i<51; i++))
    do
        sed -n '2,$p' chunk_${i}/fusion_statistics.tsv >>fusion_statistics.tsv
    done
rm -rf chunk* temp_cluster.bed

# 6. annotating and getting final fusions
echo -e "\nStep 6: Generating final fusions"
# split file to 50 chunk to redunce memory and speed up processing
lineCount=$( wc -l fusion_statistics.tsv | cut -f1 -d " " )
chunk=$( echo "$lineCount/50" | bc )
for((i=1; i<50; i++))
    do
        mkdir chunk_${i}
        head -n 1 fusion_statistics.tsv >chunk_${i}/fusion_statistics.tsv
        line_start=$[ ($i-1)*$chunk+1 ]
        line_end=$[ $i*chunk ]
        if [ $chunk -gt 0 ]; then
            sed -n "${line_start},${line_end}p" fusion_statistics.tsv >>chunk_${i}/fusion_statistics.tsv
        fi
    done
if [ $chunk -gt 0 ]; then
    sed -i '1d' chunk_1/fusion_statistics.tsv
fi
line_start=$[ 49*$chunk+1 ]
line_end=$lineCount
mkdir chunk_50
if [ $chunk -gt 0 ]; then
    head -n 1 fusion_statistics.tsv >chunk_50/fusion_statistics.tsv
fi
sed -n "${line_start},${line_end}p" fusion_statistics.tsv >>chunk_50/fusion_statistics.tsv

for i in {1..50}
    do
        cd chunk_${i}
        perl $toolDir/annotate_fusions.pl -f $filter_in_the_same_gene -a $genome_fasta $annotation_file fusion_statistics.tsv >fusions_unfiltered_by_normal.tsv
        cd ..
    done

cp chunk_1/fusions_unfiltered_by_normal.tsv ./
for((i=2; i<51; i++))
    do
        sed -n '2,$p' chunk_${i}/fusions_unfiltered_by_normal.tsv >>fusions_unfiltered_by_normal.tsv
    done
rm -rf chunk*

# 7. filter by normal junction
if [ ! -z $normal_junction_dir ]; then
    echo -e "\nStep 7: Filter by normal junction"
    cp fusions_unfiltered_by_normal.tsv temp_fusions_filtered_by_normal.tsv
    ls $normal_junction_dir | while read file
        do
            perl $toolDir/filter_fusion_by_normal_junctions.pl -a $normal_adjacent_distance -t $normal_read_count_cutoff ${normal_junction_dir}/${file} temp_fusions_filtered_by_normal.tsv >fusions_filtered_by_normal.tsv
            mv fusions_filtered_by_normal.tsv temp_fusions_filtered_by_normal.tsv
        done
    mv temp_fusions_filtered_by_normal.tsv fusions_filtered_by_normal.tsv
fi

# 8. delete the temp directory
if [ -z $normal_junction_dir ]; then
    cp fusions_unfiltered_by_normal.tsv ${output_directory}/fusions.tsv
else
    cp fusions_filtered_by_normal.tsv ${output_directory}/fusions.tsv
fi
cd ${output_directory}
rm -rf temp_output_SFyNCS
awk 'BEGIN{FS=OFS="\t"} {print $1,$2,$3,$4,$5,$6,$12,$11,$29,$30,$31,$32,$36,$37,$38,$42;}' fusions.tsv >fusions_abridged.tsv
[ -e fusions.tsv.gz ] && rm fusions.tsv.gz
[ -e fusions_abridged.tsv.gz ] && rm fusions_abridged.tsv.gz
gzip fusions.tsv
gzip fusions_abridged.tsv
cd $current_directory
echo "Finished!"


endTime=$( date +%H:%M:%S)
echo -e "END:\t$endTime"
runTime=$( runningTime $startTime $endTime )
echo -e "RUNNING:\t$runTime"
